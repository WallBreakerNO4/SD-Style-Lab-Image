{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32279928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件: 300_NAI_Styles.html\n",
      "输出文件将保存到: 300_NAI_Styles_noimg.html\n",
      "处理完成！\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def remove_data_uris(input_filepath, output_filepath=None):\n",
    "    \"\"\"\n",
    "    读取HTML文件，将所有data URI的内容替换为空字符串，然后保存为新文件。\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): 输入HTML文件的路径。\n",
    "        output_filepath (str, optional): 输出HTML文件的路径。如果未指定，\n",
    "                                          则在原文件名后添加'_no_images'。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_filepath):\n",
    "        print(f\"错误：文件不存在 {input_filepath}\")\n",
    "        return\n",
    "\n",
    "    # 如果没有指定输出文件路径，则生成一个\n",
    "    if output_filepath is None:\n",
    "        name, ext = os.path.splitext(input_filepath)\n",
    "        output_filepath = (\n",
    "            f\"{name}_no_images{ext}\"  # 比如 original.html -> original_no_images.html\n",
    "        )\n",
    "\n",
    "    print(f\"正在处理文件: {input_filepath}\")\n",
    "    print(f\"输出文件将保存到: {output_filepath}\")\n",
    "\n",
    "    # 读取整个文件内容\n",
    "    # 注意：对于180MB的文件，直接读取整个文件可能占用大量内存。\n",
    "    #      但对于替换操作，通常需要一次性读取。\n",
    "    #      如果内存不够，可以尝试逐行读取并处理（但对于跨行的data URI可能失效）。\n",
    "    #      假设180MB在现代PC内存可接受范围内。\n",
    "    try:\n",
    "        with open(input_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"警告：使用UTF-8解码失败，尝试使用latin-1或gbk。\")\n",
    "        try:\n",
    "            with open(input_filepath, \"r\", encoding=\"latin-1\") as f:\n",
    "                content = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                with open(input_filepath, \"r\", encoding=\"gbk\") as f:\n",
    "                    content = f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"错误：无法解码文件内容，请手动检查文件编码。错误信息: {e}\")\n",
    "                return\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时发生错误: {e}\")\n",
    "        return\n",
    "\n",
    "    # 正则表达式匹配data URI\n",
    "    # data:image/[^;]+;base64,[A-Za-z0-9+/=]+\n",
    "    # 解释：\n",
    "    # `data:image/` - 字面匹配 \"data:image/\"\n",
    "    # `[^;]+` - 匹配非分号的任意字符一次或多次 (MIME类型，如png, jpeg)\n",
    "    # `;base64,` - 字面匹配 \";base64,\"\n",
    "    # `[A-Za-z0-9+/=]+` - 匹配Base64编码字符集一次或多次\n",
    "    # 注意：这个正则只针对图片。如果还有其他类型的data URI（如字体、SVG），\n",
    "    #       需要调整为更通用的 `data:[^;]+;base64,[A-Za-z0-9+/=]+`\n",
    "    #       或者更通用的 `data:[^\"'\\s]+` 如果你想清除所有data URI\n",
    "    #\n",
    "    # 对于你的目的是为了去除所有图片内容，所以针对`data:image`更准确。\n",
    "    # 将匹配到的整个URI替换为 `\"\"` 或 `\"data:image/png;base64,\"` 占位符。\n",
    "    # 如果替换为空字符串，`<img>`标签的src属性就会变成`src=\"\"`，浏览器会显示一个 broken image icon。\n",
    "    # 如果你不想显示断裂图片图标，可以替换为 `src=\"#\"` 或 `src=\"about:blank\"`。\n",
    "    # 这里我们直接替换整个data URI字符串，让src属性可能变短或者为空。\n",
    "    #\n",
    "    # 修改：为了确保替换后仍是合法的src属性值，我们可以将其替换为 `data:,` (一个空白的Data URI)\n",
    "    # 或 `about:blank` （浏览器默认的空白页URI），或者一个小的空白图片Base64字符串。\n",
    "    # 最简单粗暴直接替换为空字符串 `\"\"`。\n",
    "    # 或者替换为 `\"\"`，那么 `src=\"\"` 会在某些浏览器中导致请求当前页面的路径，可能引发问题。\n",
    "    # 最佳实践是替换为 `about:blank` 或者一个非常小的透明图片占位符。\n",
    "    # 这里我们把 `data:image/...` 替换为 `data:,` （一个空内容的数据URI），它不会加载任何东西。\n",
    "\n",
    "    # 匹配 data Uri Scheme 的通用模式 (针对图片，但可以扩展)\n",
    "    # 考虑 data:image/png;base64,.....\n",
    "    # 也考虑 url() 中的 data:image/png;base64,.....\n",
    "    # 替换这些data URI为 'data:,' (一个空的data URI)\n",
    "\n",
    "    # 针对 src=\"data:image...\"\n",
    "    # pattern1 = r'src=\"data:image/[^;]+;base64,[A-Za-z0-9+/=]+\"' # 匹配整个 src=\"data:...\"\n",
    "    # 替换为 src=\"\" 或者 src=\"data:,\"\n",
    "    # processed_content = re.sub(pattern1, 'src=\"data:,\"', content)\n",
    "\n",
    "    # 更安全的做法是只替换 Base64 内容部分\n",
    "    # 这会保留 `data:image/[type];base64,` 前缀，只清空后面的数据。\n",
    "    # 这样 `src=\"data:image/png;base64,\"` 仍然是一个有效的URI，但没有实际的图片数据。\n",
    "    pattern_base64_data_only = r\"(data:image/[^;]+;base64,)([A-Za-z0-9+/=]+)\"\n",
    "    processed_content = re.sub(\n",
    "        pattern_base64_data_only, r\"\\1\", content\n",
    "    )  # 只保留前面的 `data:image/[type];base64,`\n",
    "\n",
    "    # 如果文件中还有 `url(data:image/...)` 形式的CSS背景图片等，也需要处理：\n",
    "    pattern_url_data_uri = r'url\\(\"?data:image/[^;]+;base64,[A-Za-z0-9+/=]+\"?\\)'\n",
    "    processed_content = re.sub(\n",
    "        pattern_url_data_uri, r'url(\"data:,\")', processed_content\n",
    "    )  # 替换为 url('data:,')\n",
    "\n",
    "    # 写入新文件\n",
    "    try:\n",
    "        with open(output_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(processed_content)\n",
    "        print(\"处理完成！\")\n",
    "    except Exception as e:\n",
    "        print(f\"写入文件时发生错误: {e}\")\n",
    "\n",
    "\n",
    "# 示例调用\n",
    "\n",
    "# 如果你想指定输出文件路径：\n",
    "remove_data_uris(\"300_NAI_Styles.html\", \"300_NAI_Styles_noimg.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b59d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功将数据写入到 ./300_NAI_Styles_Table.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "\n",
    "def html_table_to_csv(html_file_path, csv_file_path):\n",
    "    \"\"\"\n",
    "    Reads an HTML file containing a table and converts the table data to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        html_file_path (str): The path to the input HTML file.\n",
    "        csv_file_path (str): The path to the output CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(html_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            html_content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误：文件未找到 - {html_file_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时发生错误 {html_file_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    table = soup.find(\"table\")\n",
    "\n",
    "    if not table:\n",
    "        print(\"错误：在 HTML 文件中找不到表格。\")\n",
    "        return\n",
    "\n",
    "    # Extract header row\n",
    "    headers = []\n",
    "    thead = table.find(\"thead\")\n",
    "    if thead:\n",
    "        header_row = thead.find(\"tr\")\n",
    "        if header_row:\n",
    "            headers = [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
    "\n",
    "    # If no headers found in thead, try to infer from the first row of tbody\n",
    "    if not headers:\n",
    "        tbody = table.find(\"tbody\")\n",
    "        if tbody:\n",
    "            first_row = tbody.find(\"tr\")\n",
    "            if first_row:\n",
    "                # Assuming the first row of tbody might contain headers if thead is missing or empty\n",
    "                headers = [td.get_text(strip=True) for td in first_row.find_all(\"td\")]\n",
    "                # Optionally, you might want to remove this row from the data rows later\n",
    "                # For this specific HTML, the first row of tbody is data, so we stick to thead if possible.\n",
    "                # If thead is truly missing, you might need to manually define headers or skip the first row of tbody.\n",
    "                # Given the provided HTML, thead exists, so this fallback might not be strictly necessary but is good practice.\n",
    "                pass  # Keep the logic simple for the provided HTML structure\n",
    "\n",
    "    # Ensure we have at least the expected columns based on the provided HTML structure\n",
    "    # The provided HTML has 6 columns in the header, but we are interested in the first two data columns.\n",
    "    # Let's define the output CSV headers explicitly based on the task description (Index and Artists)\n",
    "    csv_headers = [\"Index\", \"Artists\"]\n",
    "\n",
    "    # Extract data rows\n",
    "    data_rows = []\n",
    "    tbody = table.find(\"tbody\")\n",
    "    if tbody:\n",
    "        for row in tbody.find_all(\"tr\"):\n",
    "            cells = row.find_all(\n",
    "                [\"td\", \"th\"]\n",
    "            )  # Include th in case some data is in th in tbody\n",
    "            if len(cells) > 1:  # Ensure there are at least two columns\n",
    "                index_cell = cells[0]\n",
    "                artists_cell = cells[1]\n",
    "\n",
    "                # Extract text content, handling potential <br> tags and getting text from within tags\n",
    "                index_text = index_cell.get_text(strip=True)\n",
    "                artists_text = artists_cell.get_text(\n",
    "                    separator=\", \", strip=True\n",
    "                )  # Use ', ' as separator for content within the cell\n",
    "\n",
    "                data_rows.append([index_text, artists_text])\n",
    "\n",
    "    if not data_rows:\n",
    "        print(\"在表格主体中找不到数据行。\")\n",
    "        return\n",
    "\n",
    "    # Write to CSV file\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(csv_file_path), exist_ok=True)\n",
    "\n",
    "        with open(csv_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(csv_headers)  # Write the defined headers\n",
    "            writer.writerows(data_rows)\n",
    "\n",
    "        print(f\"成功将数据写入到 {csv_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"写入 CSV 文件时发生错误 {csv_file_path}: {e}\")\n",
    "\n",
    "\n",
    "# Define input and output file paths\n",
    "html_input_file = \"./300_NAI_Styles.html\"\n",
    "csv_output_file = \"./300_NAI_Styles_Table.csv\"\n",
    "\n",
    "# Run the conversion\n",
    "html_table_to_csv(html_input_file, csv_output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SD-Style",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
